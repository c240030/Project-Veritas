{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2: Advanced Feature Engineering and Model Prototyping\n",
        "\n",
        "**Goal:** To build a comprehensive feature set and develop a robust classification pipeline, adapting to technical challenges by implementing both a primary (LLM) and a fallback (Scikit-learn) strategy.\n",
        "\n",
        "This notebook documents the full workflow for Day 2, including:\n",
        "1.  **Base Feature Engineering:** Creating foundational features from text and metadata.\n",
        "2.  **Advanced NLP Features:** Enriching the data with TF-IDF keywords and LDA topics.\n",
        "3.  **Time-based Analysis:** Calculating time deltas to detect anomalous user behavior.\n",
        "4.  **Policy Modules:** Implementing both rule-based and ML-based classifiers.\n",
        "5.  **Final Output Generation:** Saving the fully augmented dataset for Day 3's evaluation."
      ],
      "metadata": {
        "id": "mXLZ5jGqjhal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Data Loading and Sentiment Analysis\n",
        "# ===================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources for VADER\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load processed and combined data from UCSD source\n",
        "df = pd.read_csv('ucsd_delaware_reviews_combined.csv')\n",
        "\n",
        "# Initialize VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Calculate sentiment scores (ensure 'text' column has no NaN values)\n",
        "df.dropna(subset=['text'], inplace=True)\n",
        "df['sentiment_score'] = df['text'].apply(lambda text: analyzer.polarity_scores(str(text))['compound'])\n",
        "\n",
        "print(\"Successfully loaded upgraded data and calculated sentiment scores!\")\n",
        "print(df[['text', 'sentiment_score', 'category']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0hbThzNGi64",
        "outputId": "4b71c2e8-d60a-4257-d842-c0d220d905d3",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded upgraded data and calculated sentiment scores!\n",
            "                                                text  sentiment_score  \\\n",
            "0  Lived here for 3 years and enjoyed it. Locatio...           0.8176   \n",
            "1  Lived here for 3 years and enjoyed it. Locatio...           0.8176   \n",
            "2  Nice complex and awesome staff.  Maintenance i...           0.9685   \n",
            "3  Nice complex and awesome staff.  Maintenance i...           0.9685   \n",
            "4  Good places for people to live my friend lives...           0.7269   \n",
            "\n",
            "                category  \n",
            "0  ['Apartment complex']  \n",
            "1  ['Apartment complex']  \n",
            "2  ['Apartment complex']  \n",
            "3  ['Apartment complex']  \n",
            "4  ['Apartment complex']  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: Metadata and Heuristic Features\n",
        "\n",
        "We begin by extracting simple but powerful features from the existing data, such as review length, user activity, and signals indicating a real visit."
      ],
      "metadata": {
        "id": "E7qoVZc-jxV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Feature Engineering\n",
        "# ===================================================================\n",
        "\n",
        "## 1. Metadata Features\n",
        "# Review length (character count)\n",
        "df['review_length'] = df['text'].str.len()\n",
        "\n",
        "# Count number of reviews per user\n",
        "# .transform('count') assigns the count to each row of that user\n",
        "df['user_review_count'] = df.groupby('user_id')['user_id'].transform('count')\n",
        "\n",
        "# Deviation between review rating and average location rating\n",
        "# fillna(0) to handle cases without avg_rating\n",
        "df['rating_deviation'] = (df['rating'] - df['avg_rating']).fillna(0)\n",
        "\n",
        "\n",
        "## 2. \"Has Visited\" Signal Inference\n",
        "# Keywords indicating the user actually visited the location\n",
        "visit_keywords = [\n",
        "    'visited', 'went to', 'ate here', 'dined here', 'was there',\n",
        "    'stayed at', 'my visit', 'our visit', 'ordered', 'tried the'\n",
        "]\n",
        "# Create flag if text contains any keyword from the list above\n",
        "df['has_visit_keyword'] = df['text'].str.contains('|'.join(visit_keywords), case=False, na=False)\n",
        "\n",
        "print(\"Successfully created new features!\")\n",
        "# Display new columns for verification\n",
        "print(df[['user_name', 'review_length', 'user_review_count', 'rating_deviation', 'has_visit_keyword']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfpmOB1aGkQA",
        "outputId": "18b6f650-a82f-445f-80bc-4998138750e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created new features!\n",
            "        user_name  review_length  user_review_count  rating_deviation  \\\n",
            "0  Heather Carper            112                  4               0.5   \n",
            "1  Heather Carper            112                  4               0.5   \n",
            "2  STACY CLAVETTE            283                  5               0.5   \n",
            "3  STACY CLAVETTE            283                  5               0.5   \n",
            "4       Zion Hood             74                  5               0.5   \n",
            "\n",
            "   has_visit_keyword  \n",
            "0              False  \n",
            "1              False  \n",
            "2              False  \n",
            "3              False  \n",
            "4              False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Rule-Based Policy Enforcement Module\n",
        "\n",
        "Next, we build a simple rule-based module. This serves as a strong baseline and provides initial labels for our machine learning models."
      ],
      "metadata": {
        "id": "wM7jHlnxj9Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Rule-based Violation Detection\n",
        "# ===================================================================\n",
        "\n",
        "def detect_violations_rules(row):\n",
        "    flags = {}\n",
        "\n",
        "    # Policy 1: Rant without visit\n",
        "    # Condition: very negative sentiment AND no visit keywords\n",
        "    is_rant_no_visit = row['sentiment_score'] < -0.5 and not row['has_visit_keyword']\n",
        "    flags['is_rant_without_visit'] = is_rant_no_visit\n",
        "\n",
        "    # Policy 2: Irrelevant content (based on simple rules)\n",
        "    # Condition: Review is too short (could be spam or valueless)\n",
        "    is_irrelevant = row['review_length'] < 20\n",
        "    flags['is_irrelevant'] = is_irrelevant\n",
        "\n",
        "    # Add 'is_clean' flag if no violations detected\n",
        "    flags['is_clean_by_rules'] = not any([is_rant_no_visit, is_irrelevant])\n",
        "\n",
        "    return pd.Series(flags)\n",
        "\n",
        "# Apply function to the entire DataFrame\n",
        "rule_based_flags = df.apply(detect_violations_rules, axis=1)\n",
        "df = pd.concat([df, rule_based_flags], axis=1)\n",
        "\n",
        "print(\"Successfully flagged violations based on rules!\")\n",
        "# Check reviews flagged as \"rant without visit\"\n",
        "print(\"\\nReviews that are potentially 'Rant without visit':\")\n",
        "print(df[df['is_rant_without_visit'] == True][['text', 'sentiment_score', 'has_visit_keyword']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFyy9BkXGlhO",
        "outputId": "124bd185-8d6f-4702-9fff-7ddc4f4a44c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully flagged violations based on rules!\n",
            "\n",
            "Reviews that are potentially 'Rant without visit':\n",
            "                                                 text  sentiment_score  \\\n",
            "30  I love this place I have severe fibromyalgia m...          -0.5568   \n",
            "31  I love this place I have severe fibromyalgia m...          -0.5568   \n",
            "42  I run a law firm. They deposit client trust mo...          -0.8070   \n",
            "43  I run a law firm. They deposit client trust mo...          -0.8070   \n",
            "84  Took my Miter saw in to replace the handle bec...          -0.5990   \n",
            "\n",
            "    has_visit_keyword  \n",
            "30              False  \n",
            "31              False  \n",
            "42              False  \n",
            "43              False  \n",
            "84              False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Model Development - Prototyping a Classifier\n",
        "\n",
        "The primary goal was to use a Large Language Model (LLM). However, due to API limitations, we pivoted to our robust fallback plan: a Scikit-learn based machine learning model."
      ],
      "metadata": {
        "id": "jhuQ9VbSkFks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Hugging Face Authentication\n",
        "# ===================================================================\n",
        "\n",
        "from google.colab import userdata\n",
        "import huggingface_hub\n",
        "\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    huggingface_hub.login(token=hf_token)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "except Exception as e:\n",
        "    print(\"Error! Make sure you've saved 'HF_TOKEN' in Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxjr4rEcGmSp",
        "outputId": "6b18da41-6653-4805-ea67-9e110bd95d91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Machine Learning Classification with Scikit-learn\n",
        "# ===================================================================\n",
        "\n",
        "# --- PLAN B: FALLBACK USING SCIKIT-LEARN (FIXED) ---\n",
        "print(\"LLM API is not available, switching to Plan B: Using Scikit-learn.\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split # Additional import to split data\n",
        "\n",
        "# 1. Prepare training data\n",
        "def create_label(row):\n",
        "    if row['is_rant_without_visit']:\n",
        "        return 'rant_no_visit'\n",
        "    if row['is_irrelevant']:\n",
        "        return 'irrelevant'\n",
        "    return 'clean'\n",
        "\n",
        "df['rule_based_label'] = df.apply(create_label, axis=1)\n",
        "\n",
        "# Handle potential NaN values in text_clean column from the beginning\n",
        "df['text_clean'] = df['text_clean'].fillna('')\n",
        "\n",
        "# Split data into training and test sets (for evaluation in Day 3)\n",
        "# We'll train the model on 80% of the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text_clean'],\n",
        "    df['rule_based_label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['rule_based_label'] # Maintain label proportions\n",
        ")\n",
        "\n",
        "# 2. Build Pipeline\n",
        "text_clf_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english')),\n",
        "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced')),\n",
        "])\n",
        "\n",
        "# 3. Train the model\n",
        "print(\"\\nTraining Logistic Regression model...\")\n",
        "text_clf_pipeline.fit(X_train, y_train)\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# 4. Make predictions on the entire dataset (now it won't error)\n",
        "df['sklearn_classification'] = text_clf_pipeline.predict(df['text_clean'])\n",
        "\n",
        "# 5. Display results\n",
        "print(\"\\nClassification results using Scikit-learn (first 10 rows):\")\n",
        "print(df[['text', 'rule_based_label', 'sklearn_classification']].head(10))\n",
        "\n",
        "# Check if it correctly detects \"rant\" cases\n",
        "print(\"\\nChecking 'rant' cases predicted by the model:\")\n",
        "print(df[df['sklearn_classification'] == 'rant_no_visit'][['text', 'sentiment_score']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06TmEeAqGnDb",
        "outputId": "37c7deaf-4262-4216-f263-afb1945f6428"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM API is not available, switching to Plan B: Using Scikit-learn.\n",
            "\n",
            "Training Logistic Regression model...\n",
            "Training complete!\n",
            "\n",
            "Classification results using Scikit-learn (first 10 rows):\n",
            "                                                text rule_based_label  \\\n",
            "0  Lived here for 3 years and enjoyed it. Locatio...            clean   \n",
            "1  Lived here for 3 years and enjoyed it. Locatio...            clean   \n",
            "2  Nice complex and awesome staff.  Maintenance i...            clean   \n",
            "3  Nice complex and awesome staff.  Maintenance i...            clean   \n",
            "4  Good places for people to live my friend lives...            clean   \n",
            "5  Good places for people to live my friend lives...            clean   \n",
            "6                                      great layout.       irrelevant   \n",
            "7                                      great layout.       irrelevant   \n",
            "8  Positives: Elevator, dog park and maintenance....            clean   \n",
            "9  Positives: Elevator, dog park and maintenance....            clean   \n",
            "\n",
            "  sklearn_classification  \n",
            "0                  clean  \n",
            "1                  clean  \n",
            "2                  clean  \n",
            "3                  clean  \n",
            "4                  clean  \n",
            "5                  clean  \n",
            "6             irrelevant  \n",
            "7             irrelevant  \n",
            "8                  clean  \n",
            "9                  clean  \n",
            "\n",
            "Checking 'rant' cases predicted by the model:\n",
            "                                                 text  sentiment_score\n",
            "18           Quality service! Won't go anywhere else.           0.0000\n",
            "19           Quality service! Won't go anywhere else.           0.0000\n",
            "30  I love this place I have severe fibromyalgia m...          -0.5568\n",
            "31  I love this place I have severe fibromyalgia m...          -0.5568\n",
            "42  I run a law firm. They deposit client trust mo...          -0.8070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of Day 2: Next Steps\n",
        "\n",
        "We have successfully created a feature-rich dataset and a functional baseline classifier. The next steps for will involve:\n",
        "-   **Advanced NLP:** Generating more sophisticated features like topics and keywords.\n",
        "-   **Model Evaluation:** Rigorously evaluating the performance of our Scikit-learn model.\n",
        "-   **Final Output:** Saving the fully augmented dataset for the final evaluation phase."
      ],
      "metadata": {
        "id": "y3mfwIBqkMtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Install Required Packages\n",
        "# ===================================================================\n",
        "\n",
        "%pip install scikit-learn gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrLtBAQMGowj",
        "outputId": "8a68c515-e94e-4eba-d380-1b0694d2b169"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Advanced NLP - Keyword Extraction and Topic Modeling\n",
        "\n",
        "To gain a deeper understanding of the review content, we apply two advanced NLP techniques:\n",
        "-   **TF-IDF:** To extract the most important keywords from each review.\n",
        "-   **LDA (Latent Dirichlet Allocation):** To automatically discover the main underlying topics discussed across all reviews. This is crucial for identifying irrelevant content."
      ],
      "metadata": {
        "id": "_Sg07RpX47nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Keyword Extraction with TF-IDF\n",
        "# ===================================================================\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "# We'll only consider the 2000 most common words to speed up processing\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
        "\n",
        "# Fit on the entire text_clean column to learn vocabulary and IDF weights\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_clean'].fillna(''))\n",
        "# Get the list of words (features)\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "def extract_top_keywords(doc, top_n=5):\n",
        "    \"\"\"Extract top N keywords from a text based on fitted TF-IDF.\"\"\"\n",
        "    # Transform only this document\n",
        "    tfidf_vector = tfidf_vectorizer.transform([doc])\n",
        "    # Sort indices of words by TF-IDF score\n",
        "    sorted_indices = np.argsort(tfidf_vector.toarray()).flatten()[::-1]\n",
        "    # Get top N keywords\n",
        "    top_keywords = feature_names[sorted_indices[:top_n]]\n",
        "    return ', '.join(top_keywords)\n",
        "\n",
        "# Apply this function to create a new column\n",
        "# Note: This step might be slow if the dataset is large\n",
        "df['keywords'] = df['text_clean'].fillna('').apply(extract_top_keywords)\n",
        "\n",
        "print(\"Keywords extracted using TF-IDF:\")\n",
        "print(df[['text', 'keywords']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulLac45TGrrN",
        "outputId": "e20960b9-694d-4bd9-f605-bded6210dbc5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords extracted using TF-IDF:\n",
            "                                                text  \\\n",
            "0  Lived here for 3 years and enjoyed it. Locatio...   \n",
            "1  Lived here for 3 years and enjoyed it. Locatio...   \n",
            "2  Nice complex and awesome staff.  Maintenance i...   \n",
            "3  Nice complex and awesome staff.  Maintenance i...   \n",
            "4  Good places for people to live my friend lives...   \n",
            "\n",
            "                                           keywords  \n",
            "0    convenience, views, apartments, lived, enjoyed  \n",
            "1    convenience, views, apartments, lived, enjoyed  \n",
            "2  close, complex, apartments, maintenance, located  \n",
            "3  close, complex, apartments, maintenance, located  \n",
            "4                 awsome, lives, says, friend, live  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Topic Modeling with LDA - Data Preparation\n",
        "# ===================================================================\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Tokenize text (split into words)\n",
        "tokenized_data = [text.split() for text in df['text_clean'].fillna('')]\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(tokenized_data)\n",
        "# Filter out extremely rare or common words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
        "\n",
        "print(\"Data prepared for LDA topic modeling.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3YsbR2qGsiS",
        "outputId": "8767c937-aac7-4775-fe11-9e93997620d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data prepared for LDA topic modeling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Topic Modeling with LDA - Model Training and Topic Assignment\n",
        "# ===================================================================\n",
        "\n",
        "# Train LDA model to identify 5 topics\n",
        "# This step may take several minutes\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10, random_state=42)\n",
        "\n",
        "print(\"LDA model training complete. Below are the 5 main topics:\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
        "\n",
        "# Assign the main topic to each review\n",
        "def get_dominant_topic(doc):\n",
        "    bow = dictionary.doc2bow(doc.split())\n",
        "    topics = lda_model.get_document_topics(bow)\n",
        "    # Choose the topic with highest probability\n",
        "    dominant_topic = sorted(topics, key=lambda x: x[1], reverse=True)[0][0]\n",
        "    return dominant_topic\n",
        "\n",
        "df['topic'] = df['text_clean'].fillna('').apply(get_dominant_topic)\n",
        "\n",
        "print(\"\\nTopics assigned to each review:\")\n",
        "print(df[['text', 'topic']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvZpUIENGtRy",
        "outputId": "0ccf6e32-fc4c-468e-e72c-a81c6be89bbb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA model training complete. Below are the 5 main topics:\n",
            "Topic: 0 \n",
            "Words: 0.048*\"the\" + 0.035*\"to\" + 0.024*\"you\" + 0.023*\"i\" + 0.023*\"they\" + 0.023*\"and\" + 0.023*\"a\" + 0.019*\"is\" + 0.018*\"have\" + 0.014*\"of\"\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.051*\"a\" + 0.034*\"to\" + 0.031*\"of\" + 0.031*\"place\" + 0.030*\"great\" + 0.029*\"for\" + 0.025*\"and\" + 0.025*\"nice\" + 0.016*\"good\" + 0.013*\"very\"\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.093*\"the\" + 0.039*\"and\" + 0.032*\"was\" + 0.029*\"food\" + 0.024*\"is\" + 0.021*\"it\" + 0.019*\"best\" + 0.017*\"good\" + 0.015*\"love\" + 0.014*\"a\"\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.097*\"and\" + 0.090*\"great\" + 0.061*\"very\" + 0.055*\"service\" + 0.044*\"friendly\" + 0.043*\"staff\" + 0.042*\"good\" + 0.026*\"food\" + 0.021*\"helpful\" + 0.021*\"always\"\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.046*\"i\" + 0.041*\"and\" + 0.038*\"the\" + 0.037*\"was\" + 0.033*\"to\" + 0.029*\"my\" + 0.025*\"a\" + 0.016*\"me\" + 0.016*\"for\" + 0.014*\"it\"\n",
            "\n",
            "\n",
            "Topics assigned to each review:\n",
            "                                                text  topic\n",
            "0  Lived here for 3 years and enjoyed it. Locatio...      0\n",
            "1  Lived here for 3 years and enjoyed it. Locatio...      0\n",
            "2  Nice complex and awesome staff.  Maintenance i...      0\n",
            "3  Nice complex and awesome staff.  Maintenance i...      0\n",
            "4  Good places for people to live my friend lives...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.3: Time-based Features and Ad Detection\n",
        "\n",
        "To identify potential bot-like behavior, we analyze the time between consecutive reviews from the same user. We also implement a rule to detect advertisements by searching for URLs."
      ],
      "metadata": {
        "id": "BLHCbV8Z5CEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Time Delta Analysis\n",
        "# ===================================================================\n",
        "\n",
        "# Convert 'time' column (milliseconds) to datetime format\n",
        "df['datetime'] = pd.to_datetime(df['time'], unit='ms')\n",
        "\n",
        "# Sort DataFrame by user and time\n",
        "df = df.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "# Calculate time difference (in seconds) compared to the previous review by the same user\n",
        "df['time_delta_seconds'] = df.groupby('user_id')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "print(\"Calculated time differences between reviews from the same user:\")\n",
        "print(df[['user_name', 'datetime', 'time_delta_seconds']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGzUO3AiGt9b",
        "outputId": "141fad25-ff4a-4f0c-8226-b172ea79b4ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated time differences between reviews from the same user:\n",
            "          user_name                datetime  time_delta_seconds\n",
            "648     Ronald Keys 2018-03-01 15:56:48.891        0.000000e+00\n",
            "44104  Don Kelleher 2020-05-21 21:43:37.803        0.000000e+00\n",
            "14627   Tom Carroll 2020-02-08 01:04:21.369        0.000000e+00\n",
            "23922   Tom Carroll 2020-10-11 14:24:50.518        2.130243e+07\n",
            "35287  Jeff Peacock 2017-07-23 02:04:50.769        0.000000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## URL Detection for Advertisement Identification\n",
        "# ===================================================================\n",
        "\n",
        "# Create a few fake reviews containing URLs for testing\n",
        "promo_text_1 = \"Great place! visit www.mypromo.com for a 10% discount!\"\n",
        "promo_text_2 = \"I loved it, check out my blog at http://myblog.net\"\n",
        "df.loc[len(df)] = df.iloc[0] # Copy a row as a template\n",
        "df.loc[len(df)-1, 'text'] = promo_text_1\n",
        "df.loc[len(df)] = df.iloc[1]\n",
        "df.loc[len(df)-1, 'text'] = promo_text_2\n",
        "\n",
        "# Define pattern to catch URLs\n",
        "url_pattern = r'(https|http|www)[^\\s]+'\n",
        "df['has_url'] = df['text'].str.contains(url_pattern, case=False, na=False)\n",
        "\n",
        "print(f\"Number of reviews with URLs after adding samples: {df['has_url'].sum()}\")\n",
        "print(\"Reviews containing URLs:\")\n",
        "print(df[df['has_url']][['text', 'has_url']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JxfahgqGu1D",
        "outputId": "c744c050-d565-48ce-bd63-4a4074d8c23d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-48226079.py:15: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df['has_url'] = df['text'].str.contains(url_pattern, case=False, na=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews with URLs after adding samples: 7\n",
            "Reviews containing URLs:\n",
            "                                                    text  has_url\n",
            "9783   Incredibly delicious!!\\nAs always.\\nLuv the su...     True\n",
            "31808  Very disorganized staff unfriendly and unhelpf...     True\n",
            "58287  I loved it, check out my blog at http://myblog...     True\n",
            "27067  I live by this place. I don't eat Chinese but ...     True\n",
            "23844  (Translated by Google) Awwweeeessssooooomeeee ...     True\n",
            "52771  (Translated by Google) Slowwwww at the pharmac...     True\n",
            "37895  Time well spent and well worth it.\\nEvery wher...     True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 & 3: Policy Modules - Rules and Scikit-learn Fallback\n",
        "\n",
        "Now we combine all our engineered features into policy modules. We create a multi-label rule-based system and then train a Scikit-learn `LogisticRegression` model to learn from these rules, providing a robust fallback classifier."
      ],
      "metadata": {
        "id": "cXvBJWVi5J9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Multi-label Classification\n",
        "# ===================================================================\n",
        "\n",
        "# Modify function to create multiple label columns (multi-label)\n",
        "def create_multilabels(row):\n",
        "    labels = []\n",
        "    # Policy 1: Rant without visit\n",
        "    if row['sentiment_score'] < -0.5 and not row['has_visit_keyword']:\n",
        "        labels.append('rant_no_visit')\n",
        "    # Policy 2: Advertisement\n",
        "    if row['has_url']:\n",
        "        labels.append('ad')\n",
        "    # Policy 3: Irrelevant (e.g., topic 4 is considered irrelevant)\n",
        "    # Assuming after reviewing the topics above, you find topic 4 to be irrelevant\n",
        "    if row['topic'] == 4: # Replace 4 with the topic index you consider irrelevant\n",
        "        labels.append('irrelevant')\n",
        "\n",
        "    # If no labels, it's 'clean'\n",
        "    if not labels:\n",
        "        labels.append('clean')\n",
        "\n",
        "    return labels\n",
        "\n",
        "df['multilabels'] = df.apply(create_multilabels, axis=1)\n",
        "\n",
        "print(\"\\nCreated multi-label classifications:\")\n",
        "print(df[['text', 'multilabels']].tail()) # View the recently added advertisement reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g80hviVkGvg4",
        "outputId": "8cea3efa-109e-4924-d585-3650497b7952"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Created multi-label classifications:\n",
            "                                                    text multilabels\n",
            "42827  A nice beach!  Fairly busy, arrive early to ge...     [clean]\n",
            "22297  Good food, better if you eat seafood.  Friendl...     [clean]\n",
            "22299  Good food, better if you eat seafood.  Friendl...     [clean]\n",
            "19157                 Food was good, staff was friendly.     [clean]\n",
            "36516  Always great meats, great prices,  great servi...     [clean]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## LLM-based Multi-label Classification\n",
        "# ===================================================================\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Ensure you're logged in\n",
        "# huggingface_hub.login(token=userdata.get('HF_TOKEN'))\n",
        "client = InferenceClient()\n",
        "\n",
        "# Prompt improved to handle multi-label classification and request JSON output\n",
        "def classify_review_llm_multilabel(review_text, category):\n",
        "    prompt = f\"\"\"\n",
        "    As an AI assistant for Google Maps, analyze the following review for a place in the category \"{category}\".\n",
        "    A review can have one or more of the following violation labels. If no violations are found, classify it as \"clean\".\n",
        "\n",
        "    Possible Labels:\n",
        "    - \"ad\": Contains advertisements, promotions, or external links.\n",
        "    - \"irrelevant\": The content is not related to the given category.\n",
        "    - \"rant_no_visit\": A strong complaint that shows no evidence of a real visit.\n",
        "\n",
        "    Provide your answer ONLY in a valid JSON format with a single key \"labels\" which is a list of strings.\n",
        "    For example: {{\"labels\": [\"clean\"]}} or {{\"labels\": [\"ad\", \"irrelevant\"]}}.\n",
        "\n",
        "    Review Text:\n",
        "    \"{review_text}\"\n",
        "\n",
        "    JSON Output:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.text_generation(prompt, model=\"mistralai/Mistral-7B-Instruct-v0.2\", max_new_tokens=100, temperature=0.1)\n",
        "\n",
        "        json_part = response[response.find('{'):response.rfind('}')+1]\n",
        "        if json_part:\n",
        "            return json.loads(json_part)\n",
        "        else:\n",
        "            return {\"labels\": [\"error_parsing\"]}\n",
        "    except Exception as e:\n",
        "        if \"is currently loading\" in str(e):\n",
        "            print(\"Model is loading, retrying...\")\n",
        "            time.sleep(15)\n",
        "            return classify_review_llm_multilabel(review_text, category)\n",
        "        return {\"labels\": [f\"error_{str(e)}\"]}\n",
        "\n",
        "# Test on a small sample, including the advertisement reviews you just created\n",
        "sample_df_llm = df.tail(10).copy() # Take the last 10 reviews, including fake ones\n",
        "\n",
        "llm_results = sample_df_llm.apply(\n",
        "    lambda row: classify_review_llm_multilabel(row['text'], row['category']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "sample_df_llm['llm_labels'] = [res.get('labels', ['error']) for res in llm_results]\n",
        "\n",
        "print(\"\\nMulti-label classification results from LLM (Mistral):\")\n",
        "print(sample_df_llm[['text', 'multilabels', 'llm_labels']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0brh2rrGwNs",
        "outputId": "20f86229-c70e-4edc-d203-57711217a01f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-label classification results from LLM (Mistral):\n",
            "                                                    text multilabels  \\\n",
            "25777  First time I tried their food will say it wasn...     [clean]   \n",
            "39737  I have been going here for years. All the staf...     [clean]   \n",
            "35036                                            Awesome     [clean]   \n",
            "1534                              Very helpful and nice.     [clean]   \n",
            "42774  A nice beach!  Fairly busy, arrive early to ge...     [clean]   \n",
            "42827  A nice beach!  Fairly busy, arrive early to ge...     [clean]   \n",
            "22297  Good food, better if you eat seafood.  Friendl...     [clean]   \n",
            "22299  Good food, better if you eat seafood.  Friendl...     [clean]   \n",
            "19157                 Food was good, staff was friendly.     [clean]   \n",
            "36516  Always great meats, great prices,  great servi...     [clean]   \n",
            "\n",
            "                                              llm_labels  \n",
            "25777  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "39737  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "35036  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "1534   [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "42774  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "42827  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "22297  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "22299  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "19157  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n",
            "36516  [error_Model mistralai/Mistral-7B-Instruct-v0....  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Day 2 Complete: Saving the Final Augmented Dataset\n",
        "\n",
        "All feature engineering and modeling steps for Day 2 are complete. We now save the final, fully-enriched DataFrame. This file contains all the necessary data and labels for the evaluation and optimization tasks in Day 3."
      ],
      "metadata": {
        "id": "ACvmjTa-5NWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "## Save Enriched Dataset for Day 3\n",
        "# ===================================================================\n",
        "\n",
        "# Save DataFrame enriched with all features and labels\n",
        "final_output_filename = 'final_augmented_reviews_for_day3.csv'\n",
        "df.to_csv(final_output_filename, index=False)\n",
        "\n",
        "print(f\"Successfully saved final DataFrame to file '{final_output_filename}'.\")\n",
        "print(\"This is the required input for Day 3.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp36Qph6Gw6m",
        "outputId": "20d82a3e-72a6-47f2-92c3-78f8e7af9c08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved final DataFrame to file 'final_augmented_reviews_for_day3.csv'.\n",
            "This is the required input for Day 3.\n"
          ]
        }
      ]
    }
  ]
}